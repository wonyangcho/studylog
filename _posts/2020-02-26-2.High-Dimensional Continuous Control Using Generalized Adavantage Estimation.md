- ---
  title: "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
  tags:
  use_math: true
  ---


## High-Dimensional Continuous Control Using Generalized Advantage Estimation



[참고 페이지](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)



### Abstract

- 정책 기반 강화 학습의 두 가지 문제점
  - 샘플이 많이 필요하다.
  - the non-stationarity of the incoming data에도 불구 하고 안정적이고 지속적인 개선이 어렵다.
- 제안
  - $ TD(\lambda) $ 와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 bias를 희생하더라도 policy gradient estimate의 variance를 줄인다.
  - 정책과 가치 함수에 trust region optimization을 사용 한다.



### Introduction



**A key source of difficulty in RL** 

- **credit assignment** (**distal reward problem** )
  - the long time delay between actions and their positive or negative effect on rewards



**parameterized stochastic policy** 

- unbiased 
- variance
  - since the effect of an action is confounded with the effects of past and future actions. (?)

**soft-actor critic**

- bias 이지만 낮은 variance
- 높은 variance를 갖는 다면 더 많은 샘플을 사용하면 되지만 bias는 샘플이 많더라도 수렴하지 못하거나 local optimum 조차도 아닌 a poor solution에 수렴할 수 있다.



**이 논문의 contribution**

- tolerable 한 bias 수준으로 variance 를 줄이는 policy gradient estimator를 제안 한다. ( GAE ) 
- value function에도 trust region optimization method를 사용한다.
- 위의 두가지를 합쳐서 control task의 neural network policies를 효과적으로 empirically 학습할 수 있는 알고리즘을 찾는다. 이러한 결과는 high-dimensional continuous control에 RL을 사용함으로서 state of art로 확장되었다.



### Preliminaries



- initial state $ s_0 $ 는 distribution $ \rho_0 $ 에서 샘플링 된 것이다.

- 하나의 trajectory $ (s_0, a_0, s_1, a_1, …) $ 는 terminal (absorbing) state에 도달 할 때 까지 policy $ a_t \sim \pi (a_t|s_t) $ 에 따른 sampling action과 dynamics $ s_{t+1} \sim P(s_{t+1}|s_t,a_t) $  에 따른 sampling state에 의해 generate 된다.

- reward $r_t = r(s_t, a_t, s_{t+1}) $ 는 각 타임 스템프 마다 받는다.

- 목적은 expected total reward $ \sum^\infty_{t=0} r_t $  를 최대하는 것인데, 이 것은 모든 policy에 대해 finite 하다고 가정한다.

  

- 여기서 중요한 점은 $\gamma$ 를 discount parameter가 아닌 **bias-variance trade off**로 사용한다는 것이다.



- policy gradient 방법은 gradient $ g := \bigtriangledown _\theta \mathbb{E} [ \sum^\infty_{t=0}r_t] $ 를 반복적으로 추정함으로써 expected total reward를 최대화 하는 것이다.

- policy gradient는 여러가지 다른 표현이 있다. 
  $$
  g=\mathbb{E}[\sum^\infty_{t=0}\Psi\bigtriangledown_\theta log\pi_\theta(a_t|s_t)] \qquad (1)
  $$
  여기서 $\Psi_t$는 다음 중 하나 일 수 있다.

  

  1. $\sum^\infty_{t=0}r_t$ : total reward of the trajectory.
  2. $\sum^\infty_{t^\prime=t}r_t^\prime$ : reward following action $a_t$
  3. $\sum^\infty_{t^\prime=t}r_t^\prime-b(s_t)$ : baseline version of 2
  4. $Q^\pi(s_t,a_t) $ : state-action value function
  5. $A^\pi(s_t,a_t)$ : advantage function
  6. $r_t + V^\pi (s_{t+1}) - V^\pi (s_t) $ : TD residual

  

여기서 ,

$V^\pi (s_t) := \mathbb{E}_{s_{t+1}:\infty, a_t:\infty}[\sum^\infty_{l=0} r_{t+l}]\qquad Q^\pi (s_t,a_t) := \mathbb{E}_{s_{t+1}:\infty, a_t:\infty}[\sum^\infty_{l=0} r_{t+l}]\qquad (2)$ 

  $A^\pi (s_t,a_t) := Q^\pi (s_t, a_t) - V^\pi (s_t) \qquad (3)$ 입니다.



여기서 $\Psi_t$ 에 $A^\pi(s_t,a_t)$를 사용할 때 가장 낮은 variance 가 나옵니다.  (비록, 정확히 알지 못하고 추정해야 하지만.)



#### **parameter** $\gamma$

- 비록 bias가 있겠지만 delayed effects에 대응하는 reward를 down-weighting 함으로써 variance를 줄입니다. (discounted factor와 형태는 일치하지만, 의도는 다릅니다.)



discounted value function은 다음과 같습니다.

$V^\pi (s_t) := \mathbb{E}_{s_{t+1}:\infty, a_t:\infty}[\sum^\infty_{l=0} \gamma^l r_{t+l}]\qquad Q^\pi (s_t,a_t) := \mathbb{E}_{s_{t+1}:\infty, a_t:\infty}[\sum^\infty_{l=0} \gamma^lr_{t+l}]\qquad (4)$ 

  $A^\pi (s_t,a_t) := Q^\pi (s_t, a_t) - V^\pi (s_t) \qquad (5)$ 입니다.



policy gradient에 대한 discounted approximation은 다음과 같습니다.
$$
g=\mathbb{E_{s_0:\infty,a_0:\infty}}[\sum^\infty_{t=0}A^{\pi,\gamma}(s_t,a_t)\bigtriangledown_\theta log\pi_\theta(a_t|s_t)] \qquad (6)
$$



#### How to obtain biased (but not too biased) estimators for $A^{\pi,\gamma}$



**Definition 1** The estimator $ \hat{A_t} $는 $\gamma-\text{just if} $
$$
\mathbb{E_{s_0:\infty,a_0:\infty}}[\sum^\infty_{t=0}\hat{A}(s_{0:\infty},a_{0:\infty})\bigtriangledown_\theta log\pi_\theta(a_t|s_t)]=\mathbb{E_{s_0:\infty,a_0:\infty}}[\sum^\infty_{t=0}A^{\pi,\gamma}(s_{0:\infty},a_{0:\infty})\bigtriangledown_\theta log\pi_\theta(a_t|s_t)] \qquad (7)
$$
이것은 모든  $t$에 대해서 $\hat{A_t}$가 $\gamma\text{-just}$하다면 
$$
\mathbb{E_{s_0:\infty,a_0:\infty}}[\sum^\infty_{t=0}A^{\pi,\gamma}(s_{0:\infty},a_{0:\infty})\bigtriangledown_\theta log\pi_\theta(a_t|s_t)]=g^\gamma \qquad (8)
$$

가 된다.



 $\hat{A_t}$ 가 $\gamma \text{-just}$가 되려면  $\hat{A_t}$는 $Q_t$와 $b_t$로 나눌 수 있어야 한다. 여기서 $Q_t$는 trajectory variables에 종속적이지만  $\gamma$-discounted Q-function에 대해 unbiased estimator를 제공하고 $b_t$는 $a_t$ 이전에 샘플링 된 state와 action의 arbitrary function이다.



**Proposition 1** $\hat{A_t}$를 모든 $(s_t,a_t)$에 대해 $\hat{A_t} (s_{0:\infty}, a_{0:\infty}) = Q_t(s_{t:\infty},a_{t:\infty})-b_t(s_{0:t},a_{0:t-1})$ 로 표현될 수 있다면 $\mathbb{E_{s_{t+1}:\infty,a_{t+1}:\infty |s_t,a_t}}[Q_t(s_{t:\infty},a_{t:\infty})] = Q^{\pi,\gamma}(s_t,a_t)$.

이 증명은 Appendix B에 있다.



$\gamma$-just advantage estimators for $\hat{A_t}$는 다음과 같이 쓸 수 있다.

- $\sum^\infty_{l=0} \gamma^lr_{t+l}$
- $A^{\pi,\gamma}(s_t,a_t)$
- $Q^{\pi,\gamma}(s_t,a_t)$
- $r_t+\gamma V^{\pi,\gamma}(s_{t+1}) - V^{\pi,\gamma}(s_t)$