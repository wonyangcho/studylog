- ---
  title: "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
  tags:
  use_math: true
  ---


## High-Dimensional Continuous Control Using Generalized Advantage Estimation



[참고 페이지](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)



### Abstract

- 정책 기반 강화 학습의 두 가지 문제점
  - 샘플이 많이 필요하다.
  - the non-stationarity of the incoming data에도 불구 하고 안정적이고 지속적인 개선이 어렵다.
- 제안
  - $ TD(\lambda) $ 와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 bias를 희생하더라도 policy gradient estimate의 variance를 줄인다.
  - 정책과 가치 함수에 trust region optimization을 사용 한다.



### Introduction



**A key source of difficulty in RL** 

- **credit assignment** (**distal reward problem** )
  - the long time delay between actions and their positive or negative effect on rewards



**parameterized stochastic policy** 

- unbiased 
- variance
  - since the effect of an action is confounded with the effects of past and future actions. (?)

**soft-actor critic**

- bias 이지만 낮은 variance
- 높은 variance를 갖는 다면 더 많은 샘플을 사용하면 되지만 bias는 샘플이 많더라도 수렴하지 못하거나 local optimum 조차도 아닌 a poor solution에 수렴할 수 있다.



**이 논문의 contribution**

- tolerable 한 bias 수준으로 variance 를 줄이는 policy gradient estimator를 제안 한다. ( GAE ) 
- value function에도 trust region optimization method를 사용한다.
- 위의 두가지를 합쳐서 control task의 neural network policies를 효과적으로 empirically 학습할 수 있는 알고리즘을 찾는다. 이러한 결과는 high-dimensional continuous control에 RL을 사용함으로서 state of art로 확장되었다.



### Preliminaries



- initial state $ s_0 $ 는 distribution $ \rho_0 $ 에서 샘플링 된 것이다.

- 하나의 trajectory $ (s_0, a_0, s_1, a_1, …) $ 는 terminal (absorbing) state에 도달 할 때 까지 policy $ a_t \sim \pi (a_t|s_t) $ 에 따른 sampling action과 dynamics $ s_{t+1} \sim P(s_{t+1}|s_t,a_t) $  에 따른 sampling state에 의해 generate 된다.

- reward $r_t = r(s_t, a_t, s_{t+1}) $ 는 각 타임 스템프 마다 받는다.

- 목적은 expected total reward $ \sum^\infty_{t=0} r_t $  를 최대하는 것인데, 이 것은 모든 policy에 대해 finite 하다고 가정한다.

  

- 여기서 중요한 점은 $\gamma$ 를 discount parameter가 아닌 **bias-variance trade off**로 사용한다는 것이다.



- policy gradient 방법은 gradient $ g := \bigtriangledown _\theta \mathbb{E} [ \sum^\infty_{t=0}r_t] $ 를 반복적으로 추정함으로써 expected total reward를 최대화 하는 것이다.

- policy gradient는 여러가지 다른 표현이 있다. 
  $$
  g=\mathbb{E}[\sum^\infty_{t=0}\Psi\bigtriangledown_\theta log\pi_\theta(a_t|s_t)] \qquad (1)
  $$
  여기서 $\Psi_t$는 다음 중 하나 일 수 있다.

  

  1. $\sum^\infty_{t=0}r_t$ : total reward of the trajectory.
  2. $\sum^\infty_{t^\prime=t}r_t^\prime$ : reward following action $a_t$
  3. $\sum^\infty_{t^\prime=t}r_t^\prime-b(s_t)$ : baseline version of 2
  4. $Q^\pi(s_t,a_t) $ : state-action value function
  5. $A^\pi(s_t,a_t)$ : advantage function
  6. $r_t + V^\pi (s_{t+1}) - V^\pi (s_t) $ : TD residual

  

여기서 ,

$V^\pi (s_t) := \mathbb{E}_{s_{t+1}:\infty, a_t:\infty}[\sum^\infty_{l=0} r_{t+l}]\qquad Q^\pi (s_t,a_t) := \mathbb{E}_{s_{t+1}:\infty, a_t:\infty}[\sum^\infty_{l=0} r_{t+l}]\qquad (2)$ 

  $A^\pi (s_t,a_t) := Q^\pi (s_t, a_t) - V^\pi (s_t) \qquad (3)$ 입니다.



여기서 $\Psi_t$ 에 $A^\pi(s_t,a_t)$를 사용할 때 가장 낮은 variance 가 나온다.  (비록, 정확히 알지 못하고 추정해야 하지만.)



#### **parameter** $\gamma$

- 비록 bias가 있겠지만 delayed effects에 대응하는 reward를 down-weighting 함으로써 variance를 줄인다. (discounted factor와 형태는 일치하지만, 의도는 다르다.)



discounted value function은 다음과 같다.

$V^\pi (s_t) := \mathbb{E}_{s_{t+1}:\infty, a_t:\infty}[\sum^\infty_{l=0} \gamma^l r_{t+l}]\qquad Q^\pi (s_t,a_t) := \mathbb{E}_{s_{t+1}:\infty, a_t:\infty}[\sum^\infty_{l=0} \gamma^lr_{t+l}]\qquad (4)$ 

  $A^\pi (s_t,a_t) := Q^\pi (s_t, a_t) - V^\pi (s_t) \qquad (5)$ 이다.



policy gradient에 대한 discounted approximation은 다음과 같다.
$$
g^{\gamma}:=\mathbb{E_{s_0:\infty,a_0:\infty}}[\sum^\infty_{t=0}A^{\pi,\gamma}(s_t,a_t)\bigtriangledown_\theta log\pi_\theta(a_t|s_t)] \qquad (6)
$$



#### How to obtain biased (but not too biased) estimators for $A^{\pi,\gamma}$



먼저 $\gamma-\text{just if} $ 개념을 소개한다.   $g^\gamma$ 를 추정하기 위해 식(6)에서  $A^{\pi,\gamma}$ 대신에 이것을 사용할 때 bias가 생기지 않는 etimator다. 



**Definition 1** The estimator $ \hat{A_t} $는 $\gamma-\text{just if} $
$$
\mathbb{E_{s_0:\infty,a_0:\infty}}[\sum^\infty_{t=0}\hat{A}(s_{0:\infty},a_{0:\infty})\bigtriangledown_\theta log\pi_\theta(a_t|s_t)]=\mathbb{E_{s_0:\infty,a_0:\infty}}[\sum^\infty_{t=0}A^{\pi,\gamma}(s_{0:\infty},a_{0:\infty})\bigtriangledown_\theta log\pi_\theta(a_t|s_t)] \qquad (7)
$$
이것은 모든  $t$에 대해서 $\hat{A_t}$가 $\gamma\text{-just}$하다면 
$$
\mathbb{E_{s_0:\infty,a_0:\infty}}[\sum^\infty_{t=0}A^{\pi,\gamma}(s_{0:\infty},a_{0:\infty})\bigtriangledown_\theta log\pi_\theta(a_t|s_t)]=g^\gamma \qquad (8)
$$

가 된다.



 $\hat{A_t}$ 가 $\gamma \text{-just}$가 되기 위한 한가지 충분 조건은  $\hat{A_t}$는 $Q_t$와 $b_t$로 나눌 수 있어야 한다는 것이다. 여기서 $Q_t$는 trajectory variables에 종속적이지만  $\gamma$-discounted Q-function에 대해 unbiased estimator를 제공하고 $b_t$는 $a_t$ 이전에 샘플링 된 state와 action의 arbitrary function이다.



**Proposition 1** $\hat{A_t}$를 모든 $(s_t,a_t)$에 대해 $\hat{A_t} (s_{0:\infty}, a_{0:\infty}) = Q_t(s_{t:\infty},a_{t:\infty})-b_t(s_{0:t},a_{0:t-1})$ 로 표현될 수 있다면 $\mathbb{E_{s_{t+1}:\infty,a_{t+1}:\infty |s_t,a_t}}[Q_t(s_{t:\infty},a_{t:\infty})] = Q^{\pi,\gamma}(s_t,a_t)$.

이 증명은 Appendix B에 있다.



$\gamma$-just advantage estimators for $\hat{A_t}$는 다음과 같이 쓸 수 있다.

- $\sum^\infty_{l=0} \gamma^lr_{t+l}$
- $A^{\pi,\gamma}(s_t,a_t)$
- $Q^{\pi,\gamma}(s_t,a_t)$
- $r_t+\gamma V^{\pi,\gamma}(s_{t+1}) - V^{\pi,\gamma}(s_t)$



### Advantage Function Estimation



이 번 장에서는  $\hat{g} = {1 \over N} \sum^N_{n=1} \sum^\infty_{t=0} \hat{A}^n_t\bigtriangledown _\theta log \pi_\theta(a^n_t|s^n_t)$ 과 같은 형태의 policy gradient estimator를 구하기 위해 사용되는 discounted advantage function $A^{\pi,\gamma}(s_t,a_t)$ 를 정확하게 추정하는 $\hat{A_t}$  를 구하려고 한다.



$V$ 를 approximate value function 이라 하자. 그리고 $ \delta^V_t = r_t + \gamma V(s_{t+1})-V(s_t) $ 즉  discount $\gamma$ 를 갖는 $V$ 의 TD residual 이다. $\delta^V_t$ 는 action $a_t$ 의 advantage 추정값으로 생각할 수 있다. 



사실, 우리가 정확한 가치 함수 $ V=V^{\pi, \gamma} $ 를 알고 있다면 이것은   $\gamma \text{-just advantage estimator}$ 고 실제로 $ A^{\pi, \gamma} $ 의 unbiased estimator 다. 


$$
\mathbb{E}_{s_{t+1}} [\delta_t^{V^{\pi,\gamma}}] = \mathbb{E}_{s_{t+1}}[r_t+\gamma V^{\pi,\gamma}-V^{\pi,\gamma}(s_t)] =  \mathbb{E}_{s_{t+1}}[Q^{\pi,\gamma}(s_t,a_t)-V^{\pi,\gamma}(s_t)] = A^{\pi,\gamma}(s_t,a_t)\qquad (10)
$$


이 estimator는 $V=V^{\pi,\gamma}$ 에 대해 only  $\gamma-\text{just}$ 다. 그렇지 않으면 biased policy gradient estimate를 하게 된다.



다음으로 이 $\delta$ 의 $k$ 의 sum으로 생각을 해보자 이것을 $\hat{A}_t^{(k)}$ 라 하자.

(11)

(12) 

(13)

(14)



### Interpretation as Reward Shaping

### Value Function Estimation



value function을 추정하는 방법에는 여러가지가 있지만 있다. nonlinear function approximator를 사용할 때 nonlinear regression problem을 푸는 가장 간단한 형태는 다음과 같다.
$$
\text{minimize}_\phi \sum_{n=1}^N {\lVert V_\phi (s_n)-\hat{V}_n \rVert}^2 \qquad (28)
$$
여기서 $ \hat{V}_t = \sum^{\infty}_{l=0}\gamma^l r_{t+l}$ 은 reward의 discounted sum이다. 그리고 $n$ 은 trajectory의 batch에서 모든 timestamp를 index한 것이다.  이것은 Monte Carlo or TD(1)이라 한다. 



이 논문에서는 batch optimization procedure의 각 iteration에서 value function을 최적화 하기 위해 **trust region method**를 사용했다.   (가장 최근 데이터에 overfit 되는 것을 막아준다.)

먼저, $\sigma^2 = {1 \over N}\sum_{n=1}^N {\lVert V_{\phi_\text{old}} (s_n)-\hat{V}_n \rVert}^2$ 을 계산하는데 $\phi_\text{old}$ 는 opimization 하기전의 parameter vector다. 그리고 나서 다음 constrained optimization problem을 푼다. 
$$
\text{minimize}_\phi \sum_{n=1}^N {\lVert V_\phi (s_n)-\hat{V}_n \rVert}^2
$$

$$
\text{subject to} \quad {1 \over N}\sum_{n=1}^N {{{\lVert V_{\phi} (s_n)-\hat{V}_{\phi_\text{old}} \rVert}^2} \over {2\sigma^2}} \le \epsilon \qquad (29)
$$

이 제약식은 이전 value function과 새로운 value function 사이의 평균 KL divergence를 $\epsilon$ 보다 작게 제한하는 것과 동일한데, 여기서 value function은 평균 $V_\phi (s) $ 과 variace $\sigma^2$ 를 갖는 coditional Gaussian distribution으로 parameter화 되었다.

여기서, conjugate gradient algorithm을 사용해서 trust region problem에 대한 근사해를 계산했다. 특히, 다음과 같은 quadratic program을 풀게된다.
$$
\text{minimize}_\phi g^T (\phi-\phi_\text{old} )
$$

$$
\text{subject to} \quad {1 \over N}\sum_{n=1}^N (\phi-\phi_\text{old})^TH(\phi-\phi_{\text{old}}) \le \epsilon \qquad (30)
$$

여기서 $g$는 objective의 gradient이고 $H={1 \over N} \sum_n j_n j^T_n, \text{where} \  j_n=\bigtriangledown _\phi V_\phi (s_n) $ .

H는 objective의 Hessinan에 대해 "Gauss-Newton" approximation 이다. value function을 coditional probability로 해석한다면 fisher information matrix가 된다. 

계산할 때 과정은 TRPO와 동일하다.



### Experiments

이 논문에서는 다음과 같은 질문을 조사하기 위해 실험 환경을 설계했다.

1. GAE를 사용하여 episodic total reward를 최적화 할 때 $\lambda \in [0,1] $ 과 $\gamma \in[0,1] $ 이 변할때 어떤 경험적 효과가 있는지?
2. TRPO를 policy와 value function optimization에 적용했을 때 GAE를 cotrol 문제를 해결하기 위해 large neural network policy를 최적화 하는데 사용할 수 있는지?



#### policy optimization algorithm



GAE는 여러가지 다양한 policy gradient method와 함께 사용할 수 있다. 이 논문에서는 TRPO를 사용해서 policy update를 했다. TRPO는 각 iteration마다 다음 constrained optimization을 근사적으로 해결함으로써 policy를 업데이트 한다.


$$
\text{minimize}_\theta \ L_{\theta_{\text{old}}}(\theta)
$$

$$
\text{subject to} \ \bar{D}^{\theta_\text{old}}_\text{KL} (\pi_{\theta_{\text{old}}},\pi_\theta) \le \epsilon
$$

$$
\text{where}\ L_{\theta_{\text{old}}} (\theta) = {1 \over N} \sum^N_{n=1} {{ {\pi_\theta}(s_n|s_n) } \over  {\pi_{\theta_{\text{old}}}(a_n|s_n)}}\hat{A}_n
$$

$$
\bar{D}^{\theta_\text{old}}_\text{KL} (\pi_{\theta_{\text{old}}},\pi_\theta) = {1 \over N} \sum^N_{n=1} D_{\text{KL}}(\pi_{{\theta}_{\text{old}}}(\cdot|s_n)||\pi_\theta(\cdot|s_n))\qquad (31) 
$$

TRPO 논문에서 설명한 것 처럼 objective를 linearizing 하고 constraint를 quadraticizing 함으로써 이 문제를 근사적으로 해결하는데 이것은  $\theta - \theta_\text{old} \propto -F^-1g $ 방향으로 한 step 이동하는 것이다. 



최종 알고리즘은 다음과 같다.

