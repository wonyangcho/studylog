---
title: "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
tags:
use_math: true
---

## High-Dimensional Continuous Control Using Generalized Advantage Estimation



[참고 페이지](https://reinforcement-learning-kr.github.io/2018/06/23/6_gae/)



### Abstract

- 정책 기반 강화 학습의 두 가지 문제점
  - 샘플이 많이 필요하다.
  - the non-stationarity of the incoming data에도 불구 하고 안정적이고 지속적인 개선이 어렵다.
- 제안
  - $ TD(\lambda) $ 와 유사한 advantage function의 exponentially-weighted estimator를 사용하여 bias를 희생하더라도 policy gradient estimate의 variance를 줄인다.
  - 정책과 가치 함수에 trust region optimization을 사용 한다.



### Introduction



**A key source of difficulty in RL** 

- **credit assignment** (**distal reward problem** )
  - the long time delay between actions and their positive or negative effect on rewards



**parameterized stochastic policy** 

- unbiased 
- variance
  - since the effect of an action is confounded with the effects of past and future actions. (?)

**soft-actor critic**

- bias 이지만 낮은 variance
- 높은 variance를 갖는 다면 더 많은 샘플을 사용하면 되지만 bias는 샘플이 많더라도 수렴하지 못하거나 local optimum 조차도 아닌 a poor solution에 수렴할 수 있다.



**이 논문의 contribution**

- tolerable 한 bias 수준으로 variance 를 줄이는 policy gradient estimator를 제안 한다. ( GAE ) 
- value function에도 trust region optimization method를 사용한다.
- 위의 두가지를 합쳐서 control task의 neural network policies를 효과적으로 empirically 학습할 수 있는 알고리즘을 찾는다. 이러한 결과는 high-dimensional continuous control에 RL을 사용함으로서 state of art로 확장되었다.









